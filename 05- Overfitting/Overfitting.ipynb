{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasons for Overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. High Variance Low Bias: Complex models have high variance, which makes them prone to overfitting as they can learn intricate patterns in the training data that do not generalize.\n",
    "### 2. Complex Models: Deep networks with many layers and parameters can easily fit the training data, including noise.\n",
    "### 3. Insufficient Training Data: When the amount of training data is too small relative to the complexity of the model, the model may learn patterns specific to the training data.\n",
    "### 4. Noise in Training Data: Irrelevant features or mislabeled data can lead to overfitting as the model tries to learn these noisy details.\n",
    "### 5. Training for Too Long: Prolonged training can lead to the model fitting the training data very closely, including its noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques to reduce overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Increase training data.\n",
    "#### Adding More Data: Increasing the size of the training dataset can help the model learn more general patterns.\n",
    "\n",
    "### 2. Reduce model complexity\n",
    "#### Simplifying the Model: Reducing the number of hidden layers or Reducing number of neurons in hidden layers or parameters in the model to make it less complex.\n",
    "\n",
    "### 3. Regularization: Adding a penalty to the loss function for large weights. Common techniques include:\n",
    "#### • L1 Regularization: Adds the sum of the absolute values of the weights.\n",
    "#### • L2 Regularization: Adds the sum of the squared values of the weights.\n",
    "\n",
    "### 4. Cross-Validation: Using techniques like k-fold cross-validation to ensure the model performs well on different subsets of the data.\n",
    "\n",
    "### 5. Early Stopping: Monitoring the model's performance on a validation set and stopping training when performance on the validation set starts to degrade.\n",
    "\n",
    "### 6. Dropout: Randomly sets a fraction of the input units to 0 at each update during training time, which helps prevent the network from becoming too reliant on specific neurons.\n",
    "\n",
    "### 7. Batch Normalization: Normalizing the inputs of each layer to have a mean of zero and a variance of one, which can help stabilize and accelerate the training process, reducing the tendency to overfit.\n",
    "\n",
    "### 8. Weight Sharing: Using the same weights across different parts of the network, such as in convolutional layers, to reduce the number of parameters.\n",
    "\n",
    "### 9. Ensemble Methods: Combining predictions from multiple models to improve generalization. Techniques include bagging, boosting, and stacking.\n",
    "\n",
    "### 10. Data Augmentation: Generating more training data by applying random transformations such as rotations, translations, and flips to the existing data, which helps the model generalize better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Increase training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding More Data: Increasing the size of the training dataset can help the model learn more general patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming new_data and new_labels are the additional data\n",
    "X_train = np.concatenate((X_train, new_data), axis=0)\n",
    "y_train = np.concatenate((y_train, new_labels), axis=0)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reduce model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the Model: Reducing the number of hidden layers or Reducing number of neurons in hidden layers or parameters in the model to make it less complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(input_dim,))\n",
    "hl1 = Dense(32, activation='relu')(inputs)\n",
    "hl2 = Dense(32, activation='relu')(hl1)\n",
    "outputs = Dense(num_classes, activation='softmax')(hl2)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regularization: Adding a penalty to the loss function for large weights. Common techniques include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## • L1 Regularization: Adds the sum of the absolute values of the weights.\n",
    "## • L2 Regularization: Adds the sum of the squared values of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "inputs = Input(shape=(input_dim,))\n",
    "hl1 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(inputs)\n",
    "hl2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(hl1)\n",
    "outputs = Dense(num_classes, activation='softmax')(hl2)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cross-Validation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using techniques like k-fold cross-validation to ensure the model performs well on different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Early Stopping: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring the model's performance on a validation set and stopping training when performance on the validation set starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dropout: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly sets a fraction of the input units to 0 at each update during training time, which helps prevent the network from becoming too reliant on specific neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "inputs = Input(shape=(input_dim,))\n",
    "hl1 = Dense(64, activation='relu')(inputs)\n",
    "do1 = Dropout(0.5)(hl1)\n",
    "hl2 = Dense(64, activation='relu')(do1)\n",
    "do2 = Dropout(0.5)(hl2)\n",
    "outputs = Dense(num_classes, activation='softmax')(do2)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Batch Normalization: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the inputs of each layer to have a mean of zero and a variance of one, which can help stabilize and accelerate the training process, reducing the tendency to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "inputs = Input(shape=(input_dim,))\n",
    "hl1 = Dense(64, activation='relu')(inputs)\n",
    "bn1 = BatchNormalization()(hl1)\n",
    "hl2 = Dense(64, activation='relu')(bn1)\n",
    "bn2 = BatchNormalization()(hl2)\n",
    "outputs = Dense(num_classes, activation='softmax')(bn2)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Weight Sharing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the same weights across different parts of the network, such as in convolutional layers, to reduce the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "\n",
    "inputs = Input(shape=(img_height, img_width, 3))\n",
    "con1 = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "con2 = Conv2D(32, (3, 3), activation='relu')(con1)\n",
    "f1 = Flatten()(con2)\n",
    "outputs = Dense(num_classes, activation='softmax')(f1)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Ensemble Methods: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining predictions from multiple models to improve generalization. Techniques include bagging, boosting, and stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Assuming model1 and model2 are two pre-trained models\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('model1', model1),\n",
    "    ('model2', model2)\n",
    "], voting='soft')\n",
    "\n",
    "ensemble_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data Augmentation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating more training data by applying random transformations such as rotations, translations, and flips to the existing data, which helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining All Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Define the model\n",
    "inputs = Input(shape=(img_height, img_width, 3))\n",
    "con1 = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "con2 = Conv2D(32, (3, 3), activation='relu')(con1)\n",
    "bn1 = BatchNormalization()(con2)\n",
    "f1 = Flatten()(bn1)\n",
    "hl1 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(f1)\n",
    "do1 = Dropout(0.5)(hl1)\n",
    "hl2 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(do1)\n",
    "do2 = Dropout(0.5)(hl2)\n",
    "outputs = Dense(num_classes, activation='softmax')(do2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32), \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[early_stopping])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
